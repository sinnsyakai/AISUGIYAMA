import streamlit as st
import os
from dotenv import load_dotenv
from langchain_google_genai import ChatGoogleGenerativeAI
from langchain_huggingface import HuggingFaceEmbeddings
from langchain_chroma import Chroma
from langchain.chains import create_history_aware_retriever, create_retrieval_chain
from langchain.chains.combine_documents import create_stuff_documents_chain
from langchain_core.prompts import ChatPromptTemplate, MessagesPlaceholder
from langchain_core.messages import HumanMessage, AIMessage
from ingest import ingest_data
import sys
from io import StringIO

# Load environment variables
load_dotenv()

st.set_page_config(page_title="AIã™ãã‚„ã¾", page_icon="assets/new_icon.jpg")

# â–¼â–¼â–¼ ã“ã“ã«æœ€å¼·ç‰ˆCSSã‚’é…ç½®ï¼ˆä»–ã®å‡¦ç†ã‚ˆã‚Šã‚‚å…ˆã«èª­ã¿è¾¼ã¾ã›ã‚‹ï¼‰ â–¼â–¼â–¼
st.markdown("""
    <style>
    /* 1. å…¨ä½“ã®èƒŒæ™¯è‰² */
    .stApp {
        background-color: #f0fdf4;
    }
    
    /* 2. ãƒ˜ãƒƒãƒ€ãƒ¼ãƒ»ãƒ•ãƒƒã‚¿ãƒ¼ãƒ»ãƒ„ãƒ¼ãƒ«ãƒãƒ¼ãƒ»ãƒãƒ³ãƒãƒ¼ã‚¬ãƒ¼ãƒ¡ãƒ‹ãƒ¥ãƒ¼ã®å®Œå…¨éè¡¨ç¤º */
    header, footer, 
    [data-testid="stHeader"], 
    [data-testid="stFooter"], 
    [data-testid="stToolbar"], 
    [data-testid="stHeaderActionElements"],
    .stAppDeployButton,
    div[data-testid="stStatusWidget"] {
        display: none !important;
        visibility: hidden !important;
        height: 0 !important;
        width: 0 !important;
        opacity: 0 !important;
        pointer-events: none !important;
    }

    /* 3. å³ä¸‹ã®ã€ŒManage Appã€ãƒœã‚¿ãƒ³ã‚„GitHubã‚¢ã‚¤ã‚³ãƒ³å‘¨è¾ºã®å¼·åŠ›ãªæ¶ˆå» */
    /* Streamlit Cloudç‰¹æœ‰ã®è¦ç´ ã‚’ã‚¯ãƒ©ã‚¹åã®ä¸€éƒ¨ä¸€è‡´ã§ç‹™ã„æ’ƒã¡ã—ã¾ã™ */
    div[class*="viewerBadge"],
    div[class*="stAppDeployButton"],
    button[title="View app in Streamlit Cloud"],
    [data-testid="manage-app-button"] {
        display: none !important;
        visibility: hidden !important;
    }

    /* 4. ãã®ä»–ç´°ã‹ã„UIèª¿æ•´ï¼ˆæ–‡å­—è‰²ãªã©ï¼‰ */
    body, .stApp, p, div, span, li, .stTextInput input {
        color: #333333 !important;
    }
    h1, h2, h3, h4, h5, h6 {
        color: #065f46 !important;
        font-family: 'Helvetica Neue', Arial, sans-serif;
    }
    
    /* 5. ãƒãƒ£ãƒƒãƒˆãƒœãƒƒãƒˆã®å¹ãå‡ºã—ãƒ‡ã‚¶ã‚¤ãƒ³ */
    .stChatMessage {
        background-color: transparent;
    }
    [data-testid="stChatMessage"]:nth-child(odd) {
        background-color: #d1fae5;
        border-radius: 20px;
        padding: 10px;
        margin-bottom: 10px;
    }
    [data-testid="stChatMessage"]:nth-child(even) {
        background-color: #ffffff;
        border-radius: 20px;
        padding: 10px;
        margin-bottom: 10px;
        border: 2px solid #a7f3d0;
    }
    
    /* 6. ãƒœã‚¿ãƒ³ã®ãƒ‡ã‚¶ã‚¤ãƒ³ */
    .stButton > button {
        border-radius: 15px !important;
        background-color: #ffffff !important;
        color: #4b5563 !important;
        border: 1px solid #e5e7eb !important;
        box-shadow: 0 1px 3px rgba(0,0,0,0.1) !important;
    }
    .stButton > button:hover {
        border-color: #34d399 !important;
        color: #065f46 !important;
    }
    
    /* å…¥åŠ›æ¬„ã®æ ç·šã¨ã‚µã‚¤ã‚ºèª¿æ•´ */
    .stTextInput > div > div > input {
        border-radius: 20px;
        border: 2px solid #a7f3d0;
        padding: 15px 20px; /* å†…å´ã®ä½™ç™½ã‚’åºƒã’ã‚‹ */
        font-size: 16px; /* æ–‡å­—ã‚µã‚¤ã‚ºã‚’å¤§ãã */
        height: auto; /* é«˜ã•ã‚’è‡ªå‹•èª¿æ•´ */
        box-shadow: 0 2px 5px rgba(0,0,0,0.05); /* å½±ã‚’ã¤ã‘ã¦æµ®ãå‡ºã•ã›ã‚‹ */
    }
    .stTextInput > div > div > input:focus {
        border-color: #34d399; /* ãƒ•ã‚©ãƒ¼ã‚«ã‚¹æ™‚ã®è‰² */
        box-shadow: 0 0 0 2px rgba(52, 211, 153, 0.2);
    }
    </style>
""", unsafe_allow_html=True)
# â–²â–²â–² ã“ã“ã¾ã§ â–²â–²â–²

import base64

def get_image_base64(path):
    with open(path, "rb") as image_file:
        encoded_string = base64.b64encode(image_file.read()).decode()
    return encoded_string

icon_base64 = get_image_base64("assets/high_res_icon.jpg")

st.markdown(f"""
    <div style="display: flex; align-items: center; gap: 15px;">
        <img src="data:image/jpeg;base64,{icon_base64}" width="80" style="border-radius: 10px;">
        <h1 style="margin: 0; color: #065f46;">AIã™ãã‚„ã¾</h1>
    </div>
    """, unsafe_allow_html=True)
st.write("é™å²¡ã®å…ƒæ•™å¸«ã™ãã‚„ã¾ã®å‹•ç”»ãƒ»æœ¬ãªã©100ä¸‡æ–‡å­—åˆ†ã®ãƒ‡ãƒ¼ã‚¿ã‚’å­¦ç¿’ã—ãŸAIã™ãã‚„ã¾ã§ã™ã€‚å‹‰å¼·ã€é€²è·¯ã€å­è‚²ã¦ã€æ•™è‚²ã€SNSæˆ¦ç•¥ã€ãƒ“ã‚¸ãƒã‚¹ã®ãŠæ‚©ã¿ã«ç­”ãˆã¾ã™ã€‚è³ªå•å†…å®¹ã¯ãƒªã‚¢ãƒ«ã™ãã‚„ã¾ã«ã‚‚çŸ¥ã‚‰ã‚Œãªã„ã—ã€å…¬é–‹ã•ã‚Œã‚‹ã“ã¨ã‚‚ãªã„ã®ã§å®‰å¿ƒã—ã¦ç›¸è«‡ã—ã¦ãã ã•ã„ã­ã€‚")

# Sidebar for configuration
# with st.sidebar:
#     st.header("è¨­å®š")
#     # API Key is managed via secrets/env for deployment
#     try:
#         if "GOOGLE_API_KEY" in st.secrets:
#             os.environ["GOOGLE_API_KEY"] = st.secrets["GOOGLE_API_KEY"]
#     except:
#         # If secrets are not configured (local run without secrets.toml), ignore
#         pass
#     
#     # Model selection (kept for flexibility)
#     
#     model_name = st.selectbox(
#         "ãƒ¢ãƒ‡ãƒ«é¸æŠ",
#         ["gemini-flash-latest", "gemini-pro-latest", "gemini-2.0-flash-exp"],
#         index=0
#     )
#     
#     st.divider()
#     st.write("â€» åŸç¨¿ãƒ‡ãƒ¼ã‚¿ã¯ `data/` ãƒ•ã‚©ãƒ«ãƒ€ã«é…ç½®ã—ã¦ãã ã•ã„ã€‚")
#     
#     if st.button("åŸç¨¿ãƒ‡ãƒ¼ã‚¿ã‚’èª­ã¿è¾¼ã‚€ (å­¦ç¿’é–‹å§‹)"):
#         with st.spinner("åŸç¨¿ãƒ‡ãƒ¼ã‚¿ã‚’èª­ã¿è¾¼ã¿ä¸­... (åˆå›ã¯ãƒ¢ãƒ‡ãƒ«ã®ãƒ€ã‚¦ãƒ³ãƒ­ãƒ¼ãƒ‰ã«æ™‚é–“ãŒã‹ã‹ã‚Šã¾ã™)"):
#             try:
#                 # Capture stdout to show progress
#                 old_stdout = sys.stdout
#                 sys.stdout = mystdout = StringIO()
#                 
#                 ingest_data()
#                 
#                 sys.stdout = old_stdout
#                 st.success("èª­ã¿è¾¼ã¿å®Œäº†ï¼")
#                 st.expander("ãƒ­ã‚°ã‚’è¡¨ç¤º").text(mystdout.getvalue())
#                 
#                 # Clear cache to reload retriever
#                 st.cache_resource.clear()
#                 
#             except Exception as e:
#                 st.error(f"ã‚¨ãƒ©ãƒ¼ãŒç™ºç”Ÿã—ã¾ã—ãŸ: {e}")

# Hardcode model for public deployment
model_name = "gemini-flash-latest"

# Ensure API Key is loaded from secrets if available (for public deployment)
try:
    if "GOOGLE_API_KEY" in st.secrets:
        os.environ["GOOGLE_API_KEY"] = st.secrets["GOOGLE_API_KEY"]
except:
    pass

# Check for API Key
if not os.getenv("GOOGLE_API_KEY"):
    st.warning("API KeyãŒç’°å¢ƒå¤‰æ•°ã¾ãŸã¯Secretsã«è¦‹ã¤ã‹ã‚Šã¾ã›ã‚“ã§ã—ãŸã€‚")
    st.info("ãƒ­ãƒ¼ã‚«ãƒ«ã§å®Ÿè¡Œã™ã‚‹å ´åˆã¯ã€`.env` ãƒ•ã‚¡ã‚¤ãƒ«ã‚’ä½œæˆã™ã‚‹ã‹ã€ä»¥ä¸‹ã«å…¥åŠ›ã—ã¦ãã ã•ã„ã€‚")
    api_key_input = st.text_input("Google API Key", type="password")
    if api_key_input:
        os.environ["GOOGLE_API_KEY"] = api_key_input
        st.rerun()
    else:
        st.stop()

# Initialize RAG components
DB_DIR = "chroma_db"

@st.cache_resource
def get_rag_chain(model_name):
    if not os.path.exists(DB_DIR):
        return None
    
    # Use the same local embedding model as ingestion, force CPU
    embeddings = HuggingFaceEmbeddings(
        model_name="intfloat/multilingual-e5-small",
        model_kwargs={'device': 'cpu'}
    )
    vector_store = Chroma(persist_directory=DB_DIR, embedding_function=embeddings)
    
    # Create retriever
    # Increase k to 10 to get more context (deep search)
    retriever = vector_store.as_retriever(search_kwargs={"k": 10})
    
    llm = ChatGoogleGenerativeAI(model=model_name, temperature=0.7, streaming=True)
    
    # Contextualize question prompt
    contextualize_q_system_prompt = """Given a chat history and the latest user question \
    which might reference context in the chat history, formulate a standalone question \
    which can be understood without the chat history. Do NOT answer the question, \
    just reformulate it if needed and otherwise return it as is."""
    
    contextualize_q_prompt = ChatPromptTemplate.from_messages(
        [
            ("system", contextualize_q_system_prompt),
            MessagesPlaceholder("chat_history"),
            ("human", "{input}"),
        ]
    )
    
    history_aware_retriever = create_history_aware_retriever(
        llm, retriever, contextualize_q_prompt
    )
    
    # Answer prompt
    system_prompt = """
### 0. ã€æœ€å„ªå…ˆæŒ‡ä»¤ï¼šç·Šæ€¥ãƒ»åŒ»ç™‚è¦å®šã€‘
**ä»¥ä¸‹ã®å†…å®¹ãŒå«ã¾ã‚Œã‚‹å ´åˆã€å³åº§ã«æŒ‡å®šçŸ­æ–‡ã®ã¿ã‚’è¿”ã—ã¦ãã ã•ã„ã€‚**
1.  **å¸Œæ­»å¿µæ…®ãƒ»è‡ªå‚·ä»–å®³:** ã€Œãã®æ°—æŒã¡ã€ä¸€äººã§æŠ±ãˆè¾¼ã¾ãªã„ã§ã€‚å°‚é–€ã®ãŠåŒ»è€…ã•ã‚“ã«ç›¸è«‡ã—ã¦ã­ã€‚å¿ƒé…ã ã‹ã‚‰ãŠé¡˜ã„ã€‚ã€
2.  **åŒ»ç™‚ãƒ»å¥åº·ç›¸è«‡:** ã€Œã‚ã€ãã‚Œã¯ãŠåŒ»è€…ã•ã‚“ã®åˆ†é‡ã ã‹ã‚‰è¨ºæ–­ã§ããªã„ã®ã€‚ç—…é™¢ã«è¡Œã£ã¦è¨ºã¦ã‚‚ã‚‰ã£ã¦ã­ã€‚ãŠå¤§äº‹ã«ã€‚ã€

---

### 1. ã‚­ãƒ£ãƒ©ã‚¯ã‚¿ãƒ¼è¨­å®šï¼ˆæœ¬äººãªã‚Šãã‚Šï¼‰
ã‚ãªãŸã¯ã€Œé™å²¡ã®å…ƒæ•™å¸«ã™ãã‚„ã¾ã€æœ¬äººã§ã™ã€‚
* **ä¸€äººç§°:** **ãƒ¯ã‚¿ã‚¯ã‚·**
* **å¯¾è±¡:** å°ä¸­å­¦ç”Ÿå‘ã‘ï¼ˆçŸ­ãã€ã‚„ã•ã—ã„è¨€è‘‰ã§ï¼‰ã€‚
* **æ–‡é‡:** **ã‚¹ãƒãƒ›1ç”»é¢ã§ãƒ‘ãƒƒã¨èª­ã‚ã‚‹é•·ã•**ã«åã‚ã‚‹ã€‚ï¼ˆ300æ–‡å­—ç¨‹åº¦ã€‚å¿…è¦ãªå ´åˆã¯é•·æ–‡å¯ï¼‰
* **NG:** ã€Œãƒ•ã‚¡ã‚¤ãƒ«ã«ã‚ˆã‚‹ã¨ã€ã€Œè‘—æ›¸ã«ã¯ã€ç­‰ã®ç¬¬ä¸‰è€…ç›®ç·šã€‚ã™ã¹ã¦ã€Œãƒ¯ã‚¿ã‚¯ã‚·ã®è¨˜æ†¶ãƒ»ä½“é¨“ã€ã¨ã—ã¦èªã‚‹ã€‚
* **ç¦æ­¢:** å›ç­”ä¸­ã« `ã€å‡ºå…¸ï¼šãƒ•ã‚¡ã‚¤ãƒ«åã€‘` ã‚„ `[doc1]` ç­‰ã‚’è¡¨ç¤ºã—ãªã„ã“ã¨ã€‚

### 2. è©±ã—æ–¹ã¨å£ç™–ï¼ˆæŒ‡å®šãƒ«ãƒ¼ãƒ«å³å®ˆï¼‰
**æ–‡è„ˆã«åˆã‚ã›ã¦ã€ä»¥ä¸‹ã®å£ç™–ã‚’è‡ªç„¶ã«ä½¿ã„ã“ãªã—ã€ã‚¢ãƒ‰ãƒªãƒ–ã§ä¼šè©±ã—ã¦ãã ã•ã„ã€‚**

* **ã€å£ç™–ãƒªã‚¹ãƒˆã€‘**
    * **ã€Œçµè«–ï¼ã€**ï¼ˆ**ã“ã“ãã¨ã„ã†æ™‚ã ã‘ä½¿ã†ã€‚æ™®æ®µã®ä¼šè©±ã§ã¯ä½¿ã‚ãªã„ã“ã¨**ï¼‰
    * **ã€Œãªã‚“ã§ã‹ã£ã¦è¨€ã†ã¨â€¦ã€**ï¼ˆç†ç”±ã‚’è¨€ã†æ™‚ã€ç„¡ç†ã«ä½¿ã‚ãªã„ï¼‰
    * **ã€Œãƒ¤ãƒã™ãã€ã€Œãƒ„ãƒ©ã™ãã€**ï¼ˆå…±æ„Ÿãƒ»æŒ‡æ‘˜ã™ã‚‹æ™‚ï¼‰
    * **ã€Œã€œã™ãã€**ï¼ˆã‚‚ã®ã™ã”ãã€œã§ã‚ã‚‹ã¨è¨€ã†æ™‚ï¼‰
    * **ã€Œæ­£ç›´å•é¡Œã€**ï¼ˆæœ¬éŸ³ã‚’ã¶ã£ã¡ã‚ƒã‘ã‚‹æ™‚ï¼‰

* **ã€èªå°¾ã®ãƒ«ãƒ¼ãƒ«ï¼ˆæœ€é‡è¦ï¼šãƒãƒ©ãƒ³ã‚¹ï¼‰ã€‘**
    * **æ¨å¥¨:** **ä»¥ä¸‹ã®èªå°¾ã‚’ãƒãƒ©ãƒ³ã‚¹ã‚ˆãä½¿ã„åˆ†ã‘ã‚‹ã“ã¨ã€‚**
        * ã€Œã€œãªã®ã€ã€Œã€œãªã®ã­ã€ã€Œã€œã®ã€ã€Œã€œã˜ã‚ƒãªã„ï¼Ÿã€ã€Œã€œã™ãã€ã€Œã€œã ã‚ˆã­ã€ã€Œã€œã‚ˆã€
    * **ä½¿ç”¨é »åº¦ã‚’ä¸‹ã’ã‚‹ï¼ˆå¤šç”¨ç¦æ­¢ï¼‰:**
        * ã€Œã€œã—ã‚ˆã†ã­ã€ã€Œã€œã ã‚ˆã€ã€Œã€œã„ã‚‹ã‚ˆã€ï¼ˆã“ã‚Œã‚‰ãŒé€£ç¶šã—ãªã„ã‚ˆã†ã«æ³¨æ„ï¼‰
    * **ç¦æ­¢:** **ã€Œã€œã¾ã™ã€ã€Œã€œã§ã™ã‚ˆã€ã€Œã€œãªã‚“ã§ã™ã€ã°ã‹ã‚Šã«ãªã‚‹ã®ã¯çµ¶å¯¾ã«é¿ã‘ã‚‹ã“ã¨ã€‚**
    * **ç¦æ­¢:** **åŒã˜èªå°¾ãŒ2å›ä»¥ä¸Šç¶šã‹ãªã„ã‚ˆã†ã«ã™ã‚‹ã“ã¨ã€‚**
    * **å®Œå…¨ç¦æ­¢:** **ã€Œã€œãªã‚“ã ã€ã€Œã€œã—ãŸã‚“ã ã€ã¯çµ¶å¯¾ã«ä½¿ç”¨ç¦æ­¢ã€‚**
    * **æ³¨æ„:** æ–‡æœ«ã®ãƒãƒªã‚¨ãƒ¼ã‚·ãƒ§ãƒ³ã‚’è±Šã‹ã«ã—ã€å˜èª¿ã«ãªã‚‰ãªã„ã‚ˆã†ã«ã™ã‚‹ã€‚
    * **ä¸å¯§ã•:** æ±ºã—ã¦ä¹±æš´ã«ãªã‚‰ãšã€è¦ªã—ã¿ã‚„ã™ã•ã€å…ˆç”Ÿã‚‰ã—ã„å“ã®è‰¯ã•ã‚’ä¿ã¤ã€‚

### 3. æ€è€ƒãƒ—ãƒ­ã‚»ã‚¹ï¼ˆæŸ”è»Ÿãªå¯¾å¿œï¼‰
**åŸºæœ¬ã¯ã€ŒãƒŠãƒ¬ãƒƒã‚¸ï¼ˆçŸ¥è­˜ãƒ™ãƒ¼ã‚¹ï¼‰ã€ã‚’å„ªå…ˆã—ã¾ã™ãŒã€ä¼šè©±ã®è‡ªç„¶ã•ã‚’æœ€é‡è¦–ã—ã¾ã™ã€‚**

1.  **æ¤œç´¢çµæœã®ç¢ºèª:** è³ªå•ã«é–¢é€£ã™ã‚‹æƒ…å ±ãŒãƒŠãƒ¬ãƒƒã‚¸ã«ã‚ã‚‹ã‹ç¢ºèªã™ã‚‹ã€‚
2.  **åˆ¤æ–­:**
    *   **æƒ…å ±ãŒã‚ã‚‹å ´åˆ:** ãƒŠãƒ¬ãƒƒã‚¸ã®å†…å®¹ï¼ˆã™ãã‚„ã¾ã®æŒè«–ï¼‰ã‚’ä½¿ã£ã¦å›ç­”ã™ã‚‹ã€‚
    *   **æƒ…å ±ãŒãªã„ãƒ»ç„¡é–¢ä¿‚ãªå ´åˆ:** **ç„¡ç†ã«ãƒŠãƒ¬ãƒƒã‚¸ã‚’ä½¿ã‚ãšã€ã‚ãªãŸã®ä¸€èˆ¬çš„ãªçŸ¥è­˜ã¨å¸¸è­˜ã‚’ä½¿ã£ã¦ã€ã™ãã‚„ã¾å…ˆç”Ÿã¨ã—ã¦è‡ªç„¶ã«å›ç­”ã™ã‚‹ã€‚**ï¼ˆã€Œè³‡æ–™ã«ãªã„ã€ã¨ã¯è¨€ã‚ãªã„ã“ã¨ï¼‰
3.  **è‡ªç„¶ãªä¼šè©±:**
    *   æŒ¨æ‹¶ã‚„é›‘è«‡ã«ã¯ã€ãƒŠãƒ¬ãƒƒã‚¸ã‚’ä½¿ã‚ãšäººé–“ã‚‰ã—ãåå¿œã™ã‚‹ã€‚
    *   è³ªå•ã¨é–¢ä¿‚ãªã„ãƒŠãƒ¬ãƒƒã‚¸ãŒæ¤œç´¢ã•ã‚ŒãŸå ´åˆã¯ã€**ç„¡è¦–ã—ã¦**ä¼šè©±ã®æµã‚Œã‚’å„ªå…ˆã™ã‚‹ã€‚

### 4. ä¼šè©±ã®é€²ã‚æ–¹ï¼ˆè‡ªç„¶ãªå¯¾è©±ï¼‰
*   **ä¸æ˜ç¢ºãªè³ªå•ã¸ã®å¯¾å¿œï¼ˆé‡è¦ï¼‰:**
    *   ç›¸æ‰‹ã®è³ªå•ãŒæ›–æ˜§ãªå ´åˆã¯ã€**é•·ã€…ã¨è§£èª¬ã›ãšã«ã€çŸ­ãèãè¿”ã—ã¦ãã ã•ã„ã€‚**
    *   ï¼ˆè‰¯ã„ä¾‹ï¼šã€Œãã‚Œã£ã¦å…·ä½“çš„ã«ã©ã†ã„ã†ã“ã¨ï¼Ÿã€ã€Œä¾‹ãˆã°ã©ã‚“ãªæ™‚ï¼Ÿã€ï¼‰
    *   ï¼ˆæ‚ªã„ä¾‹ï¼šã€Œãã‚Œã¯å¤§å¤‰ã ã­ã€‚ä¸€èˆ¬çš„ã«ã¯ã€œã€œã¨è¨€ã‚ã‚Œã¦ã„ã‚‹ã‘ã©ã€å…·ä½“çš„ã«ã¯ã©ã†ã„ã†ã“ã¨ï¼Ÿã€â†é•·ã™ãã‚‹ï¼‰
*   **æ§‹æˆã®è‡ªç”±åŒ–:**
    *   **ã€Œçµè«–ï¼ã€ã‚„å†’é ­ã®å…±æ„Ÿã¯ã€æ¯å›å…¥ã‚Œã‚‹å¿…è¦ã¯ã‚ã‚Šã¾ã›ã‚“ã€‚** è©±ã®æµã‚Œã§è‡ªç„¶ãªå ´åˆã®ã¿ä½¿ã£ã¦ãã ã•ã„ã€‚
    *   æ¯å›åŒã˜ã‚ˆã†ãªå†’é ­ã‚„ç· ã‚ã®è¨€è‘‰ã‚’ä½¿ã‚ãªã„ã“ã¨ã€‚å‹ã«ã¨ã‚‰ã‚ã‚Œãšã€ãã®å ´ã®ä¼šè©±ã®æµã‚Œã§è‡ªç„¶ã«è¿”ã—ã¦ãã ã•ã„ã€‚
*   **ç‰¹å®šãƒ†ãƒ¼ãƒã¸ã®å¯¾å¿œ:**
    *   **ç™ºé”éšœå®³ãƒ»å­¦ç¿’éšœå®³:** ã€Œã‚ãã¾ã§ä¸€èˆ¬è«–ã ã‘ã©ã€å¿…ãšå°‚é–€å®¶ã«ç›¸è«‡ã—ã¦ã­ã€ã¨å‰ç½®ãã™ã‚‹ã€‚
    *   **å¼·ã„æ•™å¸«æ‰¹åˆ¤ãƒ»ã„ã˜ã‚:** åŒèª¿ã—ã™ããšã€ã€Œã¾ãšã¯å…ˆç”Ÿã‚„ä¿¡é ¼ã§ãã‚‹å¤§äººã«ç›¸è«‡ã—ã¦ã€ã¨ä¿ƒã™ã€‚

---

### 5. å‡ºåŠ›ãƒ¬ã‚¤ã‚¢ã‚¦ãƒˆ
**èª­ã¿ã‚„ã™ã•ã‚’æœ€å„ªå…ˆã—ã€ä»¥ä¸‹ã®ãƒ«ãƒ¼ãƒ«ã§ãƒ•ã‚©ãƒ¼ãƒãƒƒãƒˆã—ã¦ãã ã•ã„ã€‚**

1.  **è¦‹å‡ºã—ã®æ´»ç”¨:**
    * é‡è¦ãªãƒã‚¤ãƒ³ãƒˆã‚„ã€Œçµè«–ï¼ã€ã®å‰ã«ã¯ã€Markdownã® **`##`** ï¼ˆH2ç›¸å½“ï¼‰ã‚’ã¤ã‘ã¦å¤ªå­—ãƒ»å¤§æ–‡å­—ã«ã™ã‚‹ã€‚
2.  **æ”¹è¡Œã®å¾¹åº•ï¼ˆè¦‹ã‚„ã™ã•ï¼‰:**
    * **ç®‡æ¡æ›¸ãã‚„ç•ªå·ãƒªã‚¹ãƒˆï¼ˆ1. 2. 3.ï¼‰ã‚’ä½¿ã†å ´åˆã¯ã€é …ç›®ã®ç›´å¾Œã§å¿…ãšã€Œæ”¹è¡Œã€ã‚’å…¥ã‚Œã‚‹ã“ã¨ã€‚**
    * ï¼ˆæ‚ªã„ä¾‹ï¼š`1. èª­æ›¸ â†’ æœ¬ã‚’èª­ã‚€ã“ã¨`ï¼‰
    * ï¼ˆè‰¯ã„ä¾‹ï¼š`1. èª­æ›¸` (æ”¹è¡Œ) `æœ¬ã‚’èª­ã‚€ã“ã¨`ï¼‰

    ã‚³ãƒ³ãƒ†ã‚­ã‚¹ãƒˆ:
    {context}"""
    
    qa_prompt = ChatPromptTemplate.from_messages(
        [
            ("system", system_prompt),
            MessagesPlaceholder("chat_history"),
            ("human", "{input}"),
        ]
    )
    
    question_answer_chain = create_stuff_documents_chain(llm, qa_prompt)
    rag_chain = create_retrieval_chain(history_aware_retriever, question_answer_chain)
    
    return rag_chain

rag_chain = get_rag_chain(model_name)

if rag_chain is None:
    st.error("ãƒ‡ãƒ¼ã‚¿ãƒ™ãƒ¼ã‚¹ãŒè¦‹ã¤ã‹ã‚Šã¾ã›ã‚“ã€‚ã‚µã‚¤ãƒ‰ãƒãƒ¼ã®ãƒœã‚¿ãƒ³ã‹ã‚‰åŸç¨¿ãƒ‡ãƒ¼ã‚¿ã‚’èª­ã¿è¾¼ã‚“ã§ãã ã•ã„ã€‚")
    st.stop()



# Chat interface
if "messages" not in st.session_state:
    st.session_state.messages = []

# 1. Handle Chat Input
if prompt := st.chat_input("ä½•ã‹è³ªå•ã¯ã‚ã‚Šã¾ã™ã‹ï¼Ÿ"):
    st.session_state.messages.append({"role": "user", "content": prompt})

# 2. Display History
for message in st.session_state.messages:
    if message["role"] == "assistant":
        with st.chat_message(message["role"], avatar="assets/new_icon.jpg"):
            st.markdown(message["content"])
    else:
        with st.chat_message(message["role"]):
            st.markdown(message["content"])

# 3. Example Questions (Only if history is empty)
if len(st.session_state.messages) == 0:
    st.markdown("### ğŸ’¡ ã‚ˆãã‚ã‚‹è³ªå•")
    example_cols1 = st.columns(2)
    example_cols2 = st.columns(2)
    
    examples = [
        "ã™ãã‚„ã¾ã£ã¦ä½•è€…ãªã®ï¼Ÿ",
        "é€²è·¯ã«ã¤ã„ã¦æ‚©ã‚“ã§ã„ã‚‹",
        "ä¸ç™»æ ¡ã«ã¤ã„ã¦æ‚©ã‚“ã§ã„ã‚‹",
        "é›‘è«‡ã—ãŸã„"
    ]
    
    # Row 1
    with example_cols1[0]:
        if st.button(examples[0], use_container_width=True):
            st.session_state.messages.append({"role": "user", "content": examples[0]})
            st.rerun()
    with example_cols1[1]:
        if st.button(examples[1], use_container_width=True):
            st.session_state.messages.append({"role": "user", "content": examples[1]})
            st.rerun()
            
    # Row 2
    with example_cols2[0]:
        if st.button(examples[2], use_container_width=True):
            st.session_state.messages.append({"role": "user", "content": examples[2]})
            st.rerun()
    with example_cols2[1]:
        if st.button(examples[3], use_container_width=True):
            st.session_state.messages.append({"role": "user", "content": examples[3]})
            st.rerun()

# 4. Generate Response
if st.session_state.messages and st.session_state.messages[-1]["role"] == "user":
    with st.chat_message("assistant", avatar="assets/new_icon.jpg"):
        with st.spinner("è€ƒãˆä¸­..."):
            # Convert session state messages to LangChain format
            chat_history = []
            # Iterate through messages, forming pairs of HumanMessage and AIMessage
            # The last message is the current user prompt, so we exclude it from chat_history
            for i in range(0, len(st.session_state.messages) - 1):
                msg = st.session_state.messages[i]
                if msg["role"] == "user":
                    chat_history.append(HumanMessage(content=msg["content"]))
                elif msg["role"] == "assistant":
                    chat_history.append(AIMessage(content=msg["content"]))
            
            prompt = st.session_state.messages[-1]["content"]
            response_container = st.empty()
            full_response = ""
            
            try:
                # Use stream() instead of invoke()
                for chunk in rag_chain.stream({"input": prompt, "chat_history": chat_history}):
                    if "answer" in chunk:
                        full_response += chunk["answer"]
                        response_container.markdown(full_response)
                
                st.session_state.messages.append({"role": "assistant", "content": full_response})
                
            except Exception as e:
                error_msg = f"ã‚¨ãƒ©ãƒ¼ãŒç™ºç”Ÿã—ã¾ã—ãŸ: {e}"
                st.error(error_msg)
                st.session_state.messages.append({"role": "assistant", "content": "ç”³ã—è¨³ã‚ã‚Šã¾ã›ã‚“ã€‚ã‚¨ãƒ©ãƒ¼ãŒç™ºç”Ÿã—ã¾ã—ãŸã€‚"})
